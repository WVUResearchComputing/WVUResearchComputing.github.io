

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Workload Manager (SLURM) &mdash; WVU-RC 2023.10.11 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="File Transfer (Globus)" href="28.FileTransfer.html" />
    <link rel="prev" title="Software Packages" href="26.Software.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> WVU-RC
          

          
            
            <img src="../_static/ResearchComputing.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                2023.10
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="10.Introduction.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="20.QuickStart.html">Quick Start</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="21.GettingAccess.html">Getting Access</a></li>
<li class="toctree-l2"><a class="reference internal" href="22.Connect.html">Connect to the cluster (SSH)</a></li>
<li class="toctree-l2"><a class="reference internal" href="23.CommandLine.html">UNIX/Linux Command Line Interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="24.TextEditors.html">Terminal-based Text Editors (nano)</a></li>
<li class="toctree-l2"><a class="reference internal" href="25.DataStorage.html">Data Storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="26.Software.html">Software Packages</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Workload Manager (SLURM)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#slurm-concepts">SLURM Concepts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#nodes">Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#partitions">Partitions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#jobs">Jobs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#the-roles-of-a-workload-manager">The roles of a workload manager</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#resource-manager">Resource Manager</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#gathering-cluster-information">Gathering cluster information</a></li>
<li class="toctree-l3"><a class="reference internal" href="#job-submission">Job Submission</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="28.FileTransfer.html">File Transfer (Globus)</a></li>
<li class="toctree-l2"><a class="reference internal" href="29.WebInterface.html">Web Interface (Open On-Demand)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="30.BasicUsage.html">Basic Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="40.AdvancedUsage.html">Advanced Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="500.ScientificProgramming.html">Scientific Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="600.SoftAdmin.html">Software Administration</a></li>
<li class="toctree-l1"><a class="reference internal" href="700.DomainSpecific.html">Domain Specific Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="80.ClusterSpecific.html">Clusters Specifications</a></li>
<li class="toctree-l1"><a class="reference internal" href="90.References.html">References</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">WVU-RC</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="20.QuickStart.html">Quick Start</a> &raquo;</li>
        
      <li>Workload Manager (SLURM)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/text/27.WorkloadManager.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="workload-manager-slurm">
<span id="qs-workload-manager"></span><h1>Workload Manager (SLURM)<a class="headerlink" href="#workload-manager-slurm" title="Permalink to this headline">¶</a></h1>
<p>The workload manager is the software tool that makes a computer cluster appear and work like a single entity rather than an aggregate of computers on a network.
Our clusters use SLURM as their workload manager, and users must be familiar with a few commands to use an HPC cluster effectively.</p>
<p>We will describe the different commands in <a class="reference internal" href="34.WorkloadManager.html#bs-workload-manager"><span class="std std-ref">Workload Manager (SLURM)</span></a>, but if you are eager to know the list, here is a table with the commands you will use more often:</p>
<table class="colwidths-given docutils align-default" id="id1">
<caption><span class="caption-text">User Commands</span><a class="headerlink" href="#id1" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Command</p></th>
<th class="head"><p>Purpose</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">sbatch</span></code></p></td>
<td><p>Submit a batch script for later execution.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">scancel</span></code></p></td>
<td><p>Signal a job to be removed from the queue or its execution stopped.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">squeue</span></code></p></td>
<td><p>View information about jobs currently in queue or execution.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">sinfo</span></code></p></td>
<td><p>View information about nodes and partitions.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">scontrol</span></code></p></td>
<td><p>View and modify the configuration and state of jobs, nodes, and partitions.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">sacct</span></code></p></td>
<td><p>View accounting information, including data about previous jobs.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">salloc</span></code></p></td>
<td><p>Obtain an interactive job allocation</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">srun</span></code></p></td>
<td><p>Execute an application, including its allocation if needed.</p></td>
</tr>
</tbody>
</table>
<p>As you can see, the commands above deal with three main concepts: jobs, nodes, and partitions.
To be able to work effectively with SLURM, you must be familiar with these three concepts.</p>
<section id="slurm-concepts">
<h2>SLURM Concepts<a class="headerlink" href="#slurm-concepts" title="Permalink to this headline">¶</a></h2>
<p>To use SLURM effectively, we need to understand the concepts of compute node, partition, and job.
SLURM provides a set of commands to submit, cancel, and monitor jobs.
These jobs will execute on compute nodes that are organized logically in partitions.
Let us elaborate on these three concepts before submitting our first job.</p>
<section id="nodes">
<h3>Nodes<a class="headerlink" href="#nodes" title="Permalink to this headline">¶</a></h3>
<p>A High-Performance Computing cluster (HPC cluster) is made of a collection of computers. The term used for each computer is “node”.
These nodes are linked through a fast network such as Gigabit Ethernet or Infiniband.
In the case of Thorny Flat, we use the Omni-Path Architecture (OPA) as the fast network fabric.
In the case of Dolly Sods, the fabric is Infiniband.</p>
<p>The nodes in an HPC cluster are configured to serve different purposes. A typical organization of a cluster divides the nodes into four categories:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>Management nodes:</strong> These computers run management services, databases, monitoring tools, reporting applications, provisioning tools, and other related services system administrators use. Regular users have no direct access to these nodes.</p></li>
<li><p><strong>Login nodes:</strong> These computers are the machines where users log in through SSH to submit jobs and check results. Users must never use login nodes for any intense computations as many people is served by these machines, and they should be kept at a reasonable load to properly serve the users currently connected.</p></li>
<li><p><strong>Storage nodes:</strong> These computers host user files in possibly multiple filesystems. We use dedicated storage systems running distributed filesystems such as GPFS.</p></li>
<li><p><strong>Compute nodes:</strong> These are the computers where jobs run and where the computations actually take place. As most nodes on an HPC cluster are compute nodes, we refer to them simply as “nodes”. We will make the distinction when needed.</p></li>
</ul>
</div></blockquote>
</section>
<section id="partitions">
<h3>Partitions<a class="headerlink" href="#partitions" title="Permalink to this headline">¶</a></h3>
<p>In SLURM, a partition is a set of compute nodes grouped logically based on the hardware’s physical properties or job-scheduling policies.
The term “partition” can be misleading as the same compute node can belong to several partitions.
In other resource managers, partitions are called “queues” which is an alternative term also used in HPC.
In general, compute nodes are grouped based on the features shared by the nodes, such as the presence of GPUs or the amount of memory (RAM) installed. Another reason to create partitions is to manage jobs that run under different job-scheduling policies, such as priority or the amount of time a job is allowed to run on a partition.</p>
</section>
<section id="jobs">
<h3>Jobs<a class="headerlink" href="#jobs" title="Permalink to this headline">¶</a></h3>
<p>A Job is the central structure of a workload manager like SLURM.
A Job is made of one or more sequential steps, each step consisting of one or multiple parallel tasks that could be dispatched to multiple CPU cores on a single node or to several nodes on the cluster.
At a given time, many jobs exist on an HPC cluster.
A cluster like Thorny Flat or Dolly Sods usually has thousands of jobs running or in the queue.</p>
<p>Once a job is submitted, it stays pending of resources being allocated for it in the form of a fraction, one or more compute nodes. Jobs run on compute nodes until they complete the tasks they are supposed to run or the limit of time allowed for them is reached.
Once the job is finished, accounting information is stored in databases so information is preserved even if the job is no longer listed in the queue.</p>
</section>
</section>
<section id="the-roles-of-a-workload-manager">
<h2>The roles of a workload manager<a class="headerlink" href="#the-roles-of-a-workload-manager" title="Permalink to this headline">¶</a></h2>
<p>A workload manager like SLURM serves two main roles: <strong>Resource Manager</strong>  and <strong>Job Scheduler</strong>.</p>
<section id="resource-manager">
<h3>Resource Manager<a class="headerlink" href="#resource-manager" title="Permalink to this headline">¶</a></h3>
<p>The resource manager’s role is to collect information about all the computers in the cluster, their characteristics, and their current state.
A human equivalent for a resource manager is a mix of an accountant and a manager.
The resource manager role is mainly responsible for gluing an HPC cluster to appear to users as a single entity rather than a pile of computers.
A resource manager provides tools to execute tasks of an HPC cluster with several nodes and nodes with several cores as simple to use as an individual computer.</p>
<p>To better understand what a job is, consider first a simple command that return the name of the computer where it is executed. For example, on the login node execute:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$&gt; hostname
trcis001.hpc.wvu.edu
</pre></div>
</div>
<p>If we want to execute the same command on three machines, we can use SLURM command <code class="docutils literal notranslate"><span class="pre">srun</span></code> and execute:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$&gt; srun -N3 hostname
srun: job 3410 queued and waiting for resources
srun: job 3410 has been allocated resources
tcocm102.hpc.wvu.edu
tcocm101.hpc.wvu.edu
tcocm100.hpc.wvu.edu
</pre></div>
</div>
<p>The command has been executed on 3 machines, the cluster is used as a single entity and we are not interested exactly in which machines the command run as far as it executes on 3 different nodes. The whole purpose of using an HPC cluster is to have many computers to run and not be concerned exactly about which machine or machines the actual execution takes place.</p>
<p>When the amount of resources requested by all the jobs from all the users exceeds the number of resources available we need a system to prioritize the execution of the different jobs.
That is the role of a Job Scheduler</p>
<p><strong>Job Scheduler</strong></p>
<p>The algorithms behind the prioritization of jobs can become fairly sophisticated. The resources available on a cluster are permanently changing and jobs are submitted permanently.
The job scheduler has several objective functions including the maximal utilization of the cluster but also fairness among the users, preventing one user from monopolizing the cluster.</p>
<p>SLURM is a workload manager that takes both roles in its architecture. From the user’s point of view, all that you need to know is a handful of SLURM commands.
The SLURM commands that you will learn in this section will allow you to:</p>
<blockquote>
<div><ul class="simple">
<li><p>Submit jobs to the cluster, both interactive and non-interactive jobs.</p></li>
<li><p>Monitor the list of jobs running on the system</p></li>
<li><p>Learn the status and extra information for a particular job</p></li>
<li><p>Cancel jobs that have been submitted and they are either running or waiting in the queue</p></li>
<li><p>List the partitions on the cluster and their state</p></li>
</ul>
</div></blockquote>
</section>
</section>
<section id="gathering-cluster-information">
<h2>Gathering cluster information<a class="headerlink" href="#gathering-cluster-information" title="Permalink to this headline">¶</a></h2>
<p>The <cite>sinfo</cite> command on SLURM can be used to get an overview of the resources offered by the cluster.
By default, <cite>sinfo</cite> lists the partitions that are available.</p>
<p>On WVU clusters, partitions with the prefix “comm” are community resources.
Any HPC user can submit jobs to those partitions and the partitions were created differentiating the amount of RAM (small, medium [med], large, and extra large [xl]), the wall time policy for the partition (day or week) and two community partitions with GPU nodes, one for interactive jobs (comm_gpu_inter) and another for non-interactive jobs running for up to a week (comm_gpu_week).
The default queue is marked with a star (*) and it is called <cite>standby</cite>. Most compute nodes belong to this partition and jobs can run on it for up to 4 hours.
The <cite>standby</cite> partition should be used preferentially except if you are certain that 4 hours is not enough time to complete the job.</p>
<p>The command <cite>sinfo</cite> will list all the partitions and the state of the nodes for each of them. A more summarized version can be obtained with the argument <cite>-s</cite></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$&gt; info -s
PARTITION       AVAIL  TIMELIMIT   NODES(A/I/O/T) NODELIST
standby*           up    4:00:00      82/82/3/167 taicm[001-009],tarcl100,tarcs[100,200-206,300-304],tbdcx001,tbmcs[001-011,100-103],tbpcm200,tbpcs001,tcbcx100,tcdcx100,tcgcx300,tcocm[100-104],tcocs[001-064,100],tcocx[001-003],tcscm300,tjscl100,tjscm001,tmmcm[100-108],tngcm200,tpmcm[001-006],tsacs001,tsdcl[001-002],tsscl[001-002],ttmcm[100-101],tzecl[100-107],tzecs[100-115]
comm_small_day     up 1-00:00:00        57/8/0/65 tcocs[001-064,100]
comm_small_week    up 7-00:00:00        57/8/0/65 tcocs[001-064,100]
comm_med_day       up 1-00:00:00          1/4/0/5 tcocm[100-104]
comm_med_week      up 7-00:00:00          1/4/0/5 tcocm[100-104]
comm_xl_week       up 7-00:00:00          2/1/0/3 tcocx[001-003]
comm_gpu_inter     up    4:00:00         8/3/0/11 tbegq[200-202],tbmgq[001,100],tcogq[001-006]
comm_gpu_week      up 7-00:00:00          6/0/0/6 tcogq[001-006]
aei0001            up   infinite          0/8/1/9 taicm[001-009]
alromero           up   infinite        10/4/0/14 tarcl100,tarcs[100,200-206,300-304]
be_gpu             up   infinite          1/2/0/3 tbegq[200-202]
bvpopp             up   infinite          0/1/0/1 tbpcs001
cedumitrescu       up   infinite          0/0/1/1 tcdcx100
cfb0001            up   infinite          0/1/0/1 tcbcx100
cgriffin           up   infinite          1/0/0/1 tcgcx300
chemdept           up   infinite          0/4/0/4 tbmcs[100-103]
chemdept-gpu       up   infinite          1/0/0/1 tbmgq100
cs00048            up   infinite          0/1/0/1 tcscm300
jaspeir            up   infinite          0/2/0/2 tjscl100,tjscm001
jbmertz            up   infinite        11/6/0/17 tbmcs[001-011,100-103],tbmgq[001,100]
mamclaughlin       up   infinite          0/9/0/9 tmmcm[100-108]
ngarapat           up   infinite          0/1/0/1 tngcm200
pmm0026            up   infinite          0/6/0/6 tpmcm[001-006]
sbs0016            up   infinite          0/2/0/2 tsscl[001-002]
spdifazio          up   infinite          0/2/0/2 tsdcl[001-002]
tdmusho            up   infinite          0/2/0/2 ttmcm[100-101]
vyakkerman         up   infinite          1/0/0/1 tsacs001
zbetienne          up   infinite        0/24/0/24 tzecl[100-107],tzecs[100-115]
zbetienne_large    up   infinite          0/8/0/8 tzecl[100-107]
zbetienne_small    up   infinite        0/16/0/16 tzecs[100-115]
</pre></div>
</div>
<p>Now you know the partitions on the cluster and based on your knowledge of the job you can decide on which partition submit your job.
Now we will learn about the kinds of jobs that can be submitted and how to submit jobs.</p>
</section>
<section id="job-submission">
<h2>Job Submission<a class="headerlink" href="#job-submission" title="Permalink to this headline">¶</a></h2>
<p>The main purpuse of using an HPC cluster is the execution of jobs.
In particular jobs that due their characteristics are impractical to be executed on a normal desktop computer or laptop.
Such is the case of jobs that could take several hours or use significant amount of resources like multiple CPU cores or memory.</p>
<p>As we learn above an HPC has a variety of computers with particular purposes.
Computationally intese calculation must only take place on compute nodes.
Login nodes, the computers you first reach when connected to the cluster should be spared from any intense workload as these computers serve several other users and running on them will slow the machine and prevent others from executing effectively even the most simple commands.
Short post processing tasks are acceptable on login nodes.
As a rule of thumb, if a task takes more than one core or last for more than a few minutes it should run on a compute node instead of a login node.</p>
<p>There are two kind of jobs that can be executed on an HPC cluster, interactive and non-interactive jobs.
Interactive jobs are those where you receive resouces for you to use in real time, very similar to the way you use your own computer.
Interactive jobs are a good solution when you want to learn the steps needed to achieve the results you need.
Later on you can write those steps in the form of scripts and let the computer to execute them in your absence.</p>
<p>Non-interactive jobs are the solution to jobs that take hours to execute or to run several jobs on the cluster.
In non-interactive jobs you prepare an script, a recipe, indicating the computer, step by step, how to get the results that will allow you to take decisions later on or producing the final results for that level in your research.</p>
<p>Regardless of running, interactive or non-interactive jobs, SLURM, as workload manager, will decide on which machines (compute nodes) the jobs will run and will give you the tools to monitor the status of the jobs submitted.
It is time to learn the basics of submitting interactive and non-interactive jobs.</p>
<p>A very simple way of launch an interactive job is using the command <cite>srun</cite>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>trcis001:~$ srun --pty bash
srun: job 22432 queued and waiting for resources
srun: job 22432 has been allocated resources
tzecs115:~$
</pre></div>
</div>
<p>Notice that <cite>srun</cite> is actually taking a double function. From one side is creating a new job (In the case above the job with ID=22432) followed by a remote terminal session on the machine assigned to the job.
In the example above the job is requesting default values for all parameters.
The partition is set to <cite>standby</cite> which offers a walltime of 4 hours.
No selecting any number of nodes or cores will automaticaly assigned a single core on a single machine.</p>
<p>In the case of needing more resources, maybe a different partition or number of cores add extra arguments to the command line:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>trcis001:~$ srun -p standby -t 40:00 -c 4 --pty bash
</pre></div>
</div>
<p>In the example above we are explicitly selecting <cite>standby</cite> as partition, 40 minutes of walltime and 4 cores o a single compute nodes. The last argument in the srun command line must be the command to be executed. In this case, a bash session once logged into the assigned compute node.</p>
<p>The following is an example of a request for interactive job asking for 1 GPU 8 CPU coress for 2 hours:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>trcis001:~$ srun -p comm_gpu_inter -G 1 -t 2:00:00 -c 8 --pty bash
</pre></div>
</div>
<p>You can verify the assigned GPU using the command <cite>nvidia-smi</cite>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>trcis001:~$ srun -p comm_gpu_inter -G 1 -t 2:00:00 -c 8 --pty bash
srun: job 22599 queued and waiting for resources
srun: job 22599 has been allocated resources
tbegq200:~$ nvidia-smi
Wed Jan 18 13:27:01 2023
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  Off  | 00000000:3B:00.0 Off |                    0 |
| N/A   28C    P0    31W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</pre></div>
</div>
<p>The command above shows an NVIDIA A100 as the GPU assigned to us during the lifetime of the job.</p>
<p>In SLURM an interactive job can be launched with the command <cite>salloc</cite>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>trcis001:~$ salloc -N3
salloc: Pending job allocation 3506
salloc: job 3506 queued and waiting for resources
salloc: job 3506 has been allocated resources
salloc: Granted job allocation 3506
Loading git version 2.29.1 : dev/git/2.29.1
trcis001:~$
</pre></div>
</div>
<p>The command <cite>salloc</cite> will allocate resources (e.g. nodes or CPU cores), possibly with a set of constraints (e.g. number of processors per node or amount of memory per node).
<cite>salloc</cite> will allocate the resources and spawn a shell in which the <cite>srun</cite> command is used to launch parallel tasks.
Notice that <cite>salloc</cite> will return a shell on the same machine where the command <cite>salloc</cite> was executed.</p>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="28.FileTransfer.html" class="btn btn-neutral float-right" title="File Transfer (Globus)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="26.Software.html" class="btn btn-neutral float-left" title="Software Packages" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2023, West Virginia University

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>